Excellent â€” youâ€™re now in the real evaluation zone ğŸ”

Since youâ€™ve uploaded Meta, Apple, and Dell 2024 Annual Reports, we can design targeted retrieval scenarios that mirror realistic business, analyst, and AI-assistant use-cases.

Below is a categorized set of search/evaluation scenarios you can directly run against:
	â€¢	Microsoft Graph Retrieval API (if stored in SharePoint/OneDrive)
	â€¢	Your Vector DB baseline (using the same uploaded files)

â¸»

ğŸ§¾ 1ï¸âƒ£ Financial performance & key metrics

Scenario ID	Query	Purpose / What to Observe
F1	â€œWhat was Metaâ€™s total revenue in 2024?â€	Numeric accuracy; ability to locate tables or narrative.
F2	â€œApple operating income 2024 vs 2023â€	Comparison and cross-year reasoning.
F3	â€œDellâ€™s gross margin percentage for FY2024â€	Extraction from financial statement tables.
F4	â€œWhat was Appleâ€™s EPS (earnings per share)?â€	Precision in retrieving inline financial metrics.
F5	â€œWhich segment contributed most to Metaâ€™s 2024 revenue?â€	Section recognition â€” should surface â€œFamily of Appsâ€ or similar.

Evaluation focus:
âœ… Table parsing quality, âœ… numerical snippet accuracy, âœ… section navigation (MD&A vs tables).

â¸»

ğŸ’¼ 2ï¸âƒ£ Business strategy & outlook

Scenario ID	Query	Goal
S1	â€œWhat are Appleâ€™s strategic priorities for 2025?â€	Semantic understanding of forward-looking statements.
S2	â€œMeta 2024 cost reduction or restructuring plansâ€	Find risk/expense management narrative.
S3	â€œDellâ€™s growth opportunities in AI or Edge computingâ€	Semantic matching of technology trends.
S4	â€œAppleâ€™s capital allocation strategyâ€	Retrieval of financial policy or cash-return notes.
S5	â€œMeta managementâ€™s view on advertising marketâ€	Evaluate text comprehension of qualitative discussions.


â¸»

âš™ï¸ 3ï¸âƒ£ Technology & innovation highlights

Scenario ID	Query	Description
T1	â€œApple innovation in silicon or chip designâ€	Should find sections about M3, A17, etc.
T2	â€œMetaâ€™s advancements in AI or Metaverse in 2024â€	Keyword vs concept recall.
T3	â€œDellâ€™s R&D expenditureâ€	Numeric retrieval + section title understanding.
T4	â€œHow does Apple describe its sustainability in product design?â€	Multi-word semantic comprehension.


â¸»

ğŸŒ 4ï¸âƒ£ Sustainability, ESG, and corporate responsibility

Scenario ID	Query	Goal
E1	â€œApple carbon footprint reduction effortsâ€	Concept recall (ESG context).
E2	â€œMetaâ€™s data center sustainability goalsâ€	Retrieves non-financial section.
E3	â€œDellâ€™s diversity and inclusion initiativesâ€	Phrase expansion (â€œdiversityâ€, â€œinclusionâ€, â€œequal opportunityâ€).
E4	â€œApple renewable energy investmentsâ€	Keyword + concept retrieval.

Compare:
Does Retrieval API surface â€œEnvironmental Responsibilityâ€ sections automatically?
Does your Vector DB require chunk-level metadata to find it?

â¸»

âš–ï¸ 5ï¸âƒ£ Risk factors and management discussion

Scenario ID	Query	What to test
R1	â€œMajor risks mentioned by Apple in 2024 reportâ€	Ability to return risk summary.
R2	â€œMeta cybersecurity or data privacy risksâ€	Term sensitivity and synonyms.
R3	â€œDell supply chain disruptions or dependenciesâ€	Topic recall.
R4	â€œApple litigation or regulatory risksâ€	Keyword + semantic sensitivity.


â¸»

ğŸ‘©â€ğŸ’¼ 6ï¸âƒ£ Governance and management

Scenario ID	Query	Goal
G1	â€œWho are Metaâ€™s board members?â€	Named entity extraction.
G2	â€œAppleâ€™s executive compensation discussionâ€	Multi-page section retrieval.
G3	â€œDell audit committee responsibilitiesâ€	Section recognition.
G4	â€œCEO letter highlights in Apple 2024 reportâ€	Page position recall (front matter).


â¸»

ğŸ“Š 7ï¸âƒ£ Cross-company comparisons (multi-document RAG)

Scenario ID	Query	Intent
C1	â€œCompare Meta and Apple total revenue growth in 2024â€	Multi-doc summarization.
C2	â€œWhich company spent more on R&D?â€	Retrieve both metrics from each report.
C3	â€œWhich company had higher operating margin?â€	Structured reasoning test.
C4	â€œWhich of Meta, Apple, Dell emphasized AI in their 2024 report?â€	Semantic clustering.

Measure:
ğŸ”¹ Multi-document merging quality
ğŸ”¹ Latency when retrieving across multiple sources
ğŸ”¹ Need for deduplication or re-ranking

â¸»

ğŸ§ª 8ï¸âƒ£ Analyst-style composite queries

Scenario ID	Query	Testing aspect
A1	â€œSummarize Appleâ€™s 2024 financial highlights and risk factorsâ€	Long semantic query; test summarization readiness.
A2	â€œList all references to artificial intelligence in all reportsâ€	High recall test.
A3	â€œFind every mention of privacy regulationâ€	Keyword + context.
A4	â€œShow every table containing revenue breakdownâ€	Structural retrieval precision.


â¸»

ğŸ§© 9ï¸âƒ£ Non-English / variant phrasing (for robustness)

Scenario ID	Query	Why it matters
L1	â€œingresos totales de Apple 2024â€ (Spanish for â€œtotal incomeâ€)	Multilingual embeddings test.
L2	â€œMeta chiffre dâ€™affaires 2024â€ (French for revenue)	Cross-language semantic search.
L3	â€œDell bÃ©nÃ©fices net 2024â€	Check translation normalization.


â¸»

ğŸ§® 10ï¸âƒ£ Evaluation sheet (recommended columns)

Scenario ID	Query	Source(s)	Retrieval Top-3 Match	Vector DB Top-3 Match	Correct? (Y/N)	Latency (ms)	Notes / Observations


This structure lets you score both systems (1â€“5 scale for relevance, accuracy, freshness, explainability).

â¸»

âš™ï¸ Optional metrics to record
	â€¢	Precision@3: how many of top 3 hits truly relevant
	â€¢	Snippet clarity: short, human-readable vs long text block
	â€¢	Latency: average over 5 runs
	â€¢	Coverage: fraction of unique queries that yield at least one result
	â€¢	Security: if access-controlled files hidden (if relevant)

â¸»

Would you like me to generate a ready-to-download Excel (.xlsx) file pre-filled with all these scenarios and scoring columns so you can log results while testing both Retrieval API and your Vector DB?
